{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ScrapeGoogle git.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLQdnh08HBue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from newspaper import Config\n",
        "from newspaper import Article\n",
        "from textblob import TextBlob\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "from urllib.request import urlopen\n",
        "\n",
        "#Initial page to get results\n",
        "page = 0\n",
        "final_list = []\n",
        "#Date in format mm/dd/yyyy\n",
        "initdate = \"4/16/2020\"\n",
        "enddate = \"4/17/2020\"\n",
        "query = \"Data Science\" \n",
        "limit = 4\n",
        "#This header is to get the real results\n",
        "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}    \n",
        "#Next three lines work with the newspaper library, to ensure scrape the webpage\n",
        "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
        "config = Config()\n",
        "config.browser_user_agent = user_agent\n",
        "\n",
        "#Iterate through all the results in the search \n",
        "for x in range(0, limit):\n",
        "    #creates the url to get results from google\n",
        "    url = \"http://www.google.com/search?q=\" + query + \"&tbs=cdr:1,cd_min:\" + initdate + \",cd_max:\" + enddate + \"&sa=N&biw=1845&bih=977&dpr=1&start=\" + str(page)\n",
        "    page = page + 10\n",
        "    #Next three lines get all the html tags for that set of results\n",
        "    source_code = requests.get(url, headers = headers)\n",
        "    article_content = source_code.content\n",
        "    soup = BeautifulSoup(article_content, 'html5lib')\n",
        "    #This class has the link to urls\n",
        "    body = soup.find_all('div', class_='dbsr')\n",
        "    #If tags found\n",
        "    if len(body) > 0:\n",
        "        #Iterate through all the tags\n",
        "        for desc in soup.find(\"div\",{\"class\":\"dbsr\"}):\n",
        "            #gets the actual url to webpage\n",
        "            strurl = desc.get('href')\n",
        "            if strurl is not None:\n",
        "              #initializes newspaper library\n",
        "              article = Article(strurl, config=config, language='en')\n",
        "              #When can't get the text, throws an exception \n",
        "              try:\n",
        "                  #Next four lines get the text of the article\n",
        "                  article.download()\n",
        "                  article.parse()\n",
        "                  #article.nlp()\n",
        "                  finaltext = article.text\n",
        "                  #Delete spaces from blank results\n",
        "                  finaltext = finaltext.strip()\n",
        "                  #If finaltext is a string\n",
        "                  if finaltext:\n",
        "                      #Filter only results in english language\n",
        "                      if(TextBlob(finaltext).detect_language() == \"en\"):\n",
        "                        #Delete all special characters from string and add carriage return\n",
        "                        final_list.append(re.sub(r'\\W+', ' ', finaltext) + '\\n')\n",
        "              except:\n",
        "                  pass\n",
        "\n",
        "#Results are written to file \n",
        "with open('/content/gdrive/My Drive/Data.txt', 'w') as f:\n",
        "  f.writelines(final_list)\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}